{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1c4c0f-dba3-4fa3-ba55-d8bd3538330f",
   "metadata": {},
   "source": [
    "# From Notebook to Pipeline\n",
    "\n",
    "To bring a machine learning model into production we want to automate as much as possible, in a reproducible way.\n",
    "Therefore, production grade code should be in a proper versioned repository like git and not in a notebook.\n",
    "\n",
    "In this tutorial, we are going to turn a prototype from a notebook into a pipeline on azure machine learning.\n",
    "\n",
    "The goal is:\n",
    "* turn an existing model into a service from which other services can use to make predictions\n",
    "* extend model training so that it can run independently of your machine in the cloud and training can be automated further\n",
    "\n",
    "This is the first step on the way to create a production grade ML application in the cloud.\n",
    "\n",
    "Not part of this tutorial is the integration of the azure machine learning pipelines into a CI/CD pipeline.\n",
    "If you are interested what a CI/CD pipeline is, you can ask me and I will show you an example.\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "Azure Machine Learning python sdk gives us a relatively simple interface to run model training jobs and create deployments for inference.\n",
    "\n",
    "You can think of it as a job scheduling tool.\n",
    "Additionally to the code that trains a model and serves it, we will write job scheduling code. We will write code that creates and runs training jobs, creates and deploys  REST endpoints and creates docker images.\n",
    "\n",
    "**We assume that you already have an azure machine learning workspace.**\n",
    "\n",
    "\n",
    "## Install Environment\n",
    "\n",
    "Before starting you should install all required dependencies locally. We will do so in a virtual environment:\n",
    "```bash\n",
    "python -m venv azml-tutorial-venv\n",
    "```\n",
    "\n",
    "Activate the virtual environment.\n",
    "On unix like systems:\n",
    "```bash\n",
    "source azml-tutorial-env/bin/activate\n",
    "```\n",
    "On Windows:\n",
    "```ps1\n",
    ".\\azml-tutorial-env\\Scripts\\activate\n",
    "```\n",
    "\n",
    "\n",
    "Then register this venv with jupyter so we can use it within the notebook.\n",
    "Do so by running this code inside the activated venv:\n",
    "```bash\n",
    "python -m ipykernel install --user --name=azml-tutorial\n",
    "```\n",
    "\n",
    "Restart the notebook with the kernel `azml-tutorial` before you continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915abf1b-d262-4297-8178-d755eeed9885",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect to Azure Machine Learning\n",
    "\n",
    "We will create the client that handles all requests to azure machine learning.\n",
    "### Authenticate\n",
    "There are different methods to authenticate. Choose the one that works for you:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ae194-54f4-4d50-b77f-f4484ddf6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential, AzureCliCredential, InteractiveBrowserCredential\n",
    "\n",
    "token_url = \"https://management.azure.com/.default\"\n",
    "tenant_id = \"\" # set this to the id of your tenant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd25643",
   "metadata": {},
   "source": [
    "Depending on your circumstances you can use different ways of authenticating to Azure Machine Learning. Here are a few:\n",
    "\n",
    "```python\n",
    "credential = DefaultAzureCredential();\n",
    "credential.get_token(token_url, tenant_id=tenant_id);\n",
    "\n",
    "credential = AzureCliCredential();\n",
    "credential.get_token(token_url);\n",
    "\n",
    "# via browser\n",
    "credential = InteractiveBrowserCredential(tenant_id=tenant_id);\n",
    "credential.get_token(token_url);\n",
    "\n",
    "print(\"created the following credential: \", credential)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fbdb8",
   "metadata": {},
   "source": [
    "When running the notebook on you local machine it might be easier to authenticate via the browser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa038464",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = InteractiveBrowserCredential(tenant_id=tenant_id);\n",
    "credential.get_token(token_url);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6087bb09",
   "metadata": {},
   "source": [
    "When you are running the notebook from azure machine learning, you can authenticate first in the terminal via `az login` and then continue with the `AzureCliCredential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba36b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credential = AzureCliCredential();\n",
    "credential.get_token(token_url);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d3a1a-e985-4d87-817f-665c17eda1d3",
   "metadata": {},
   "source": [
    "### Create a client to connect to the workspace\n",
    "This is the central object to interact with Azure Machine Learning programatically.\n",
    "\n",
    "Please fill it in so that it matches your worspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc53ef-9769-4d09-9f1d-658d7b02a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id='',\n",
    "    resource_group_name='',\n",
    "    workspace_name='',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60855298-e9bd-486b-91a3-86e8052857b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(ml_client.data.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813622ee",
   "metadata": {},
   "source": [
    "**You don't need to go through the Environments section for the tutorial to work.**\n",
    "You can skip it an look at it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f2725-d7d9-4bba-a89d-195be0fe6247",
   "metadata": {},
   "source": [
    "## Run Jobs\n",
    "We will now try to give you a basic understanding of how to run jobs and how they work.\n",
    "A job will run the script you specify (or any command available from the environment). \n",
    "When you schedule a job it will get a vm and start it with the specified environment, upload the script to azure and run the command you have specified.\n",
    "\n",
    "A job can have inputs and outputs. the azure storage assets referenced in the inputs and outputs will be mounted into the filesystem of the VM of the job. Therefore, your script can read from the `Input` paths write to an `Output` paths as if it were ordinary files and directories. \n",
    "\n",
    "Here a simple example of how you can run the script `train_jonb.py` on azure machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9ff7b-4e77-4cc1-a7f2-8657a2c4a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "input_data = {\"dataset\": Input(type=AssetTypes.URI_FOLDER, path=\"azureml:name-of-dataset:1\")}\n",
    "output_data = {'model': Output(type=AssetTypes.CUSTOM_MODEL)}\n",
    "\n",
    "train_job = command(\n",
    "    code=\"./\",\n",
    "    command=\"pip install -r requirements.txt && python train_job.py --input-asset ${{inputs.dataset}} --output-asset ${{outputs.model}}\",\n",
    "    environment=\"AzureML-ACPT-pytorch-1.12-py39-cuda11.6-gpu:3\",\n",
    "    compute=\"cheap-standard-d1-v2\",\n",
    "    inputs=input_data,\n",
    "    outputs=output_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a66a9",
   "metadata": {},
   "source": [
    "From the code above, we can see, that\n",
    "* we need a dataset called `name-of-dataset` needs to exist.\n",
    " * Create in the azure machine learning sutudio an example dataset or take the name of an existing one.\n",
    "* we need a compute cluster\n",
    " * change the line so it matches your compute cluster or create one via the studio UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82756f-40ce-49c8-8b92-fc7c5a276368",
   "metadata": {},
   "source": [
    "Now we create the script `train_job.py`. Play around with it and make some changes and run the job to get used to how it works. \n",
    "\n",
    "Running the following cell will overwrite `train_job.py` so make sure you do not change it locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d51350-f5fb-468d-a9a6-605ce57e2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_job.py\n",
    "import argparse\n",
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Read command line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-asset\", type=str)\n",
    "    parser.add_argument(\"--output-asset\", type=str)\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_args()\n",
    "print(\"input_asset:\", args.input_asset)\n",
    "print(\"output_asset:\", args.output_asset)\n",
    "\n",
    "print(\"input dataset files:\", os.listdir(args.input_asset))\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, dummy):\n",
    "        self.dummy = dummy\n",
    "\n",
    "model = Model(\"dummy model.\")\n",
    "        \n",
    "# save you model within the args.output_asset path\n",
    "\n",
    "##os.makedirs(args.output_asset+\"model/\", exist_ok=True)\n",
    "with open(args.output_asset+\"/model.pickle\", \"wb\") as fh:\n",
    "    cloudpickle.dump(model, fh) # with joblib you can save almost any python object to disk\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6035c03",
   "metadata": {},
   "source": [
    "You can run the script locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7073ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run train_job.py --input-asset ./ --output-asset ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441575c4",
   "metadata": {},
   "source": [
    "Now we run the job in the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133cfdd3-42fd-420b-91be-b874104f3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = ml_client.jobs.create_or_update(train_job)\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd553dc",
   "metadata": {},
   "source": [
    "The link leads you to the currently running job in Azure ML Studio.\n",
    "Check it out and watch the job starting.\n",
    "You can look at the logs unter `Outputs + Logs` one of the shows the logs generated by our script.\n",
    "\n",
    "### Tasks:\n",
    "* Make yourself familiar with the job interface you can see the code, inputs and outputs\n",
    "* Check the logs of the job, you should find the output of the print statements in one of the logfiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937882e",
   "metadata": {},
   "source": [
    "## Task: Train a real Model\n",
    "You can now use our job running capability to create a model.\n",
    "Create a script similar to `train_job.py` that trains your model and save it into the directory you specified in the `outputs`.\n",
    "* take the code from the notebook that defines the model and trains it and put it into `your_train_job.py` or give it some other name\n",
    "* for now, train the model with the minimal amount of data necessary\n",
    "* define any parameters of this script that you may want to change via commandline arguments\n",
    "* before running the job on azure, run `your_train_job.py` from commandline with the necessary arguments locally\n",
    " * weed out errors\n",
    "* set the arguments in `command()` and define the input and output dataset accordingly\n",
    "* save the model into the `outputs` if joblib fails, \n",
    "* run the job and check if it runs as intended\n",
    "* check that the model output is created by looking at the outputs of the jobs\n",
    "* register the model you created manually via the studio ui (this could also be done via the sdk)\n",
    " * give the model a proper name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd360b",
   "metadata": {},
   "source": [
    "## Inference\n",
    "The next step is to run our model as a service and make predictions.\n",
    "\n",
    "Azure Machine learning allows us to run a model as a REST service by creating a simple wrapper of the model.\n",
    "\n",
    "**Make sure that you registered your model** You can do this via the studio UI. Select the job and there `+Register Model`.\n",
    "\n",
    "We will do it in the following.\n",
    "Read the comments to understand how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241359a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "import cloudpickle # a bit better than joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def init():\n",
    "    global model # this variable can be accessed outside this function\n",
    "\n",
    "    model_dir = os.getenv(\"AZUREML_MODEL_DIR\") #when the script runs on azure, it will have this environment variable, it contains the path to the model in the VM file system\n",
    "    if model_dir == None:\n",
    "        model_dir = \"\" # define here the path to the model on your machine if you want to test the script locally\n",
    "    model_path = os.path.join(model_dir, \"model/model.pickle\") # specify here the exact path to the model file inside the storage\n",
    "    with open(model_path, \"rb\") as fh:\n",
    "        model = cloudpickle.load(fh) # load the model, you can also use joblib\n",
    "\n",
    "        \n",
    "def run(input_data):\n",
    "    data = json.loads(input_data) # interpret the input to the rest service as json\n",
    "    # parse the json according to your needs and feed the input to the model\n",
    "    # for example you could encode the image data as base64 encoded string:\n",
    "    # # read the string:\n",
    "    # image = base64.decodestring(data['image'])\n",
    "    # This might not be the architecturally best way of using the service in a real live system.\n",
    "    # I'm happy to discuss how to handle the image data more properly :-)\n",
    "    return {\"answer\": 42, \"input\": data} # here we send the input back and add THE answer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #the code here you could use to test the functions above on your machine\n",
    "    init()\n",
    "    print(run('''{\"question\": \"Answer to the Ultimate Question of Life, The Universe, and Everything\"}'''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1383e7",
   "metadata": {},
   "source": [
    "Try out the script by running it locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ed751",
   "metadata": {},
   "source": [
    "When running the script on azure machine learning it will run `init()` on startup and `run()` every time the endpoint is called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba0d10",
   "metadata": {},
   "source": [
    "### Create the inference endpoint and deploy the script\n",
    "We will create a endpoint that is managed by azure machine learning. After the endpoint was created there will be a representation of it in the sudio UI where you can try the model out and generate code to call the endpoint from the internet as well as manage the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d02fb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model, ManagedOnlineEndpoint, ManagedOnlineDeployment, CodeConfiguration\n",
    "\n",
    "# If you want to setup the endpoint locally you need to set local=True\n",
    "# for this to work you need docker installed and running on your machine\n",
    "local=False \n",
    "online_endpoint_name = \"endpoint-service-test\"\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Test Endpoint for MLOps Tutorial\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "\n",
    "endpoint_poller = ml_client.begin_create_or_update(endpoint, local=local)\n",
    "\n",
    "if local: # if you want to upload a model from your machine set local to true\n",
    "    model = Model(path=\"model/model.pickle\")\n",
    "else:\n",
    "    model=ml_client.models.get(name=\"dummy-model\", version=\"1\") # reference the registered model by its name and version\n",
    "\n",
    "code_config = CodeConfiguration(\n",
    "    code=\"./\", scoring_script=\"inference.py\" # change if you want to run another script\n",
    ")\n",
    "\n",
    "# here you could also take a custom environment for inference that already has the exact dependencies you need\n",
    "environment = \"AzureML-ACPT-pytorch-1.12-py39-cuda11.6-gpu:3\"\n",
    "\n",
    "# you could have multiple deployments per endpoint\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=environment,\n",
    "    code_configuration=code_config,\n",
    "    instance_type=\"Standard_F2s_v2\", #change here if you need a gpu for inference\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "if not local:\n",
    "    endpoint_poller.result() # wait for the endpoint to be created\n",
    "\n",
    "deployment_poller = ml_client.begin_create_or_update(blue_deployment, local=local)\n",
    "deployment_poller.result() #wait for the deployment to be finished\n",
    "\n",
    "# blue deployment gets 100% traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint, local=runner_config.local)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19301cb9",
   "metadata": {},
   "source": [
    "Now go to the endpoint in the UI and send a test request according to the input format defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39ae7a",
   "metadata": {},
   "source": [
    "## Task: Create your own Model Endpoint\n",
    "* cpoy `inference.py` to `your_inference.py` and adjust it according to your needs\n",
    "* load your model\n",
    "* adjust `run()` such that it takes the input data the way you need it\n",
    "  * for example read it from a base64 encoded image object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee59b5-5c50-404e-844b-c6da4f63afa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environments\n",
    "If you want to create your own environments you can continue here.\n",
    "\n",
    "The role of environments is to manage and define the contents of the virtual machine where your code will run.\n",
    "\n",
    "It should contain all dependencies of the job at hand.\n",
    "\n",
    "### Types\n",
    "Two types of environments where your jobs will run:\n",
    "- existing environment\n",
    "- custom environment\n",
    "  - conda environment\n",
    "  - docker environment\n",
    "\n",
    "Use a custom environment when:\n",
    "- you have dependencies that take a lot of time to install\n",
    "- you need non-python packages or system libraries (via docker)\n",
    "\n",
    "\n",
    "With a custom environment the dependencies are installed once into the docker container. Custom environments can be based on an existing environment.\n",
    "\n",
    "### Existing Environment\n",
    "When you are using an existing environment, you can also have your own dependencies, but they will be installed every time you are running a job.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bbfd8-abdf-4085-9d87-e3e244e9ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "job_with_existing_env_with_custom_deps = command(\n",
    "    code=\"./\",\n",
    "    command=\"pip install -r requirements.txt && python script.py\",\n",
    "    environment=\"AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu:35\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9378c58-6e29-4b76-a465-770b804bb597",
   "metadata": {},
   "source": [
    "This will install the dependencies listed in requeirements every time you run the job.\n",
    "You can look up existing environments in Azure Machine Learning Studio under \"Environments\".\n",
    "\n",
    "You could run this job with: \n",
    "```python\n",
    "ml_client.environments.create_or_update(existing_env_with_custom_deps)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f620ca1-6483-447c-be89-9f53a8aeda0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom Environments\n",
    "Create an environment that contains the dependencies you need.\n",
    "In general this will build a docker container with everything that your app will need.\n",
    "\n",
    "There is a simple and a advanced way of creating an environment:\n",
    "- by supplying conda dependnecies\n",
    "- by creating your own docker file with whatever dependencies you will need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c7457-1f92-4767-9e97-1685322bc3a9",
   "metadata": {},
   "source": [
    "To create a custom environment from conda dependencies you need to reference your \"environment.yml\" file containing the conda packages you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ba0d5",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml.entities import Environment\n",
    "conda_env = Environment(\n",
    "    conda_file=\"environment.yml\",\n",
    "    name=\"tutorial-ubuntu2004-py310-cpu\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    description=\"conda environment\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c91a59-b687-42bf-a4f3-539e174482dc",
   "metadata": {},
   "source": [
    "To actually build the environment you need to send it to the workspace:\n",
    "```pathon\n",
    "ml_client.environments.create_or_update(conda_env)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83446f-c857-4bc1-bcdb-269d4d03c019",
   "metadata": {},
   "source": [
    "The most flexibility you have with the Docker Environment. Here you can specify the path to your `Dockerfile`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7478e-d92f-4d56-ba3f-a1744690e66e",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml.entities import BuildContext\n",
    "docker_env = Environment(\n",
    "    build=BuildContext(path=\"path/to/directory/where/dockerfile/is/located\", dockerfile_path=\"Dockerfile\"),\n",
    "    name=\"tutorial-advanced-ubuntu2004-py310-cpu\",\n",
    "    description=\"advanced environment\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ef3ef-ecd9-40c0-804b-4c44e6094312",
   "metadata": {},
   "source": [
    "With this you can create your custom docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436702a-85f2-4b1a-bef6-543afe48b6e2",
   "metadata": {},
   "source": [
    "### Create Project Environment\n",
    "\n",
    "We will now create a custom environment for our project. In the notebook we are using pip to install packages. It would be easier to create a conda environment, but this would need testing first.\n",
    "\n",
    "Therefore, we are going to use pip and install the packages into a custom docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "docker_env = Environment(\n",
    "    build=BuildContext(path=\"generated/docker\", dockerfile_path=\"Dockerfile\"),\n",
    "    name=\"tutorial-advanced-ubuntu2004-py310-cpu\",\n",
    "    description=\"advanced environment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10280e",
   "metadata": {},
   "source": [
    "Let us now create a corresponding `Dockerfile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b205cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "# this file is mostly a copy of the Dockerfile of the image \"AzureML-ACPT-pytorch-1.12-py39-cuda11.6-gpu\". You can find it under \"Environments\" on Azure Machine Learning Sutdio\n",
    "FROM mcr.microsoft.com/azureml/aifx/stable-ubuntu2004-cu116-py39-torch1121:biweekly.202211.1\n",
    "\n",
    "# Install pip dependencies\n",
    "RUN pip install 'ipykernel~=6.0' \\\n",
    "                'azureml-core==1.47.0' \\\n",
    "\t\t\t\t'azureml-dataset-runtime==1.47.0' \\\n",
    "                'azureml-defaults==1.47.0' \\\n",
    "\t\t\t\t'azure-ml==0.0.1' \\\n",
    "\t\t\t\t'azure-ml-component==0.9.15.post2' \\\n",
    "                'azureml-mlflow==1.47.0' \\\n",
    "\t\t'azureml-contrib-services==1.47.0' \\\n",
    "\t\t        'azureml-contrib-services==1.47.0' \\\n",
    "                'torch-tb-profiler~=0.4.0' \\\n",
    "\t\t\t\t'py-spy==0.3.12' \\\n",
    "\t\t        'debugpy~=1.6.3'\n",
    "\n",
    "RUN pip install \\\n",
    "        azure-ai-ml==0.1.0b5 \\\n",
    "        MarkupSafe==2.1.1 \\\n",
    "\t    regex \\\n",
    "\t    pybind11\n",
    "\n",
    "# Inference requirements\n",
    "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20220607.v1 /artifacts /var/\n",
    "RUN /var/requirements/install_system_requirements.sh && \\\n",
    "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
    "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
    "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
    "    rm -f /etc/nginx/sites-enabled/default\n",
    "ENV SVDIR=/var/runit\n",
    "ENV WORKER_TIMEOUT=400\n",
    "EXPOSE 5001 8883 8888\n",
    "\n",
    "# support Deepspeed launcher requirement of passwordless ssh login\n",
    "\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y openssh-server openssh-client\n",
    "RUN mkdir -p /root/.ssh\n",
    "RUN mkdir /var/run/sshd\n",
    "RUN ssh-keygen -t rsa -f /root/.ssh/id_rsa\n",
    "RUN sed -i 's/PermitRootLogin without-password/PermitRootLogin yes/' /etc/ssh/sshd_config\n",
    "RUN sed 's@session\\\\s*required\\\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd\n",
    "RUN chmod 700 /root/.ssh/\n",
    "RUN touch /root/.ssh/config;echo -e \"Port 1043\\n StrictHostKeyChecking no\\n  UserKnownHostsFile=/dev/null\" > /root/.ssh/config\n",
    "RUN echo \"Port 1043\" >> /etc/ssh/sshd_config\n",
    "RUN chmod 600 /root/.ssh/config\n",
    "RUN touch /root/.ssh/authorized_keys && chmod 600 /root/.ssh/authorized_keys\n",
    "RUN cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys\n",
    "EXPOSE 1043 1043\n",
    "CMD [\"/usr/sbin/sshd\", \"-D\"]\n",
    "\n",
    "#here our command to install our requirements:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agroscope-tutorial",
   "language": "python",
   "name": "agroscope-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
